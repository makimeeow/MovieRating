{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e383200b",
   "metadata": {},
   "source": [
    "Pipeline with one–rating-per-movie test split using CSR for the rating matrix\n",
    "1. Load raw data\n",
    "2. Split events into train/test (one test rating per movie)\n",
    "3. Build sparse movie×user rating matrix on TRAIN events\n",
    "4. Generate per-movie features from TRAIN\n",
    "5. Cluster movies (dimensionality reduction)\n",
    "6. Build event-level feature arrays & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score\n",
    ")\n",
    "import subprocess\n",
    "\n",
    "random_state = 13119140\n",
    "rng = np.random.default_rng(random_state)\n",
    "\n",
    "# %% 1) LOAD RAW DATA\n",
    "\n",
    "movie_ids = []\n",
    "user_ids  = []\n",
    "ratings   = []\n",
    "dates     = []\n",
    "\n",
    "with open(\"/Users/serenahan/Downloads/dataSet/data.txt\", \"r\") as f:\n",
    "    current_mid = None\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.endswith(\":\"):\n",
    "            current_mid = int(line[:-1])\n",
    "        else:\n",
    "            u, r, d = line.split(\",\")\n",
    "            movie_ids.append(current_mid)\n",
    "            user_ids.append(int(u))\n",
    "            ratings.append(int(r))\n",
    "            dates.append(np.datetime64(d))\n",
    "\n",
    "movie_ids = np.array(movie_ids, dtype=int)\n",
    "user_ids  = np.array(user_ids,  dtype=int)\n",
    "ratings   = np.array(ratings,   dtype=int)\n",
    "dates     = np.array(dates,     dtype=\"datetime64[D]\")\n",
    "\n",
    "# Load movie titles & release years\n",
    "mids, rel_years, titles = [], [], []\n",
    "with open(\"/Users/serenahan/Downloads/dataSet/movieTitles.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.rstrip(\"\\n\").split(\",\", 2)\n",
    "        if len(parts) != 3:\n",
    "            continue\n",
    "        mid_str, date_str, title_str = parts\n",
    "        try:\n",
    "            mid = int(mid_str)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        mids.append(mid)\n",
    "        try:\n",
    "            yr = datetime.strptime(date_str, \"%Y-%m-%d\").year\n",
    "        except:\n",
    "            yr = np.nan\n",
    "        rel_years.append(yr)\n",
    "        titles.append(title_str.strip())\n",
    "\n",
    "mids      = np.array(mids,      dtype=int)\n",
    "rel_years = np.array(rel_years, dtype=float)\n",
    "titles    = np.array(titles,    dtype=object)\n",
    "\n",
    "# release‐year lookup\n",
    "year_map = {mid: yr for mid, yr in zip(mids, rel_years)}\n",
    "\n",
    "# %% 2) SPLIT EVENTS INTO TRAIN/TEST (one test rating per movie)\n",
    "\n",
    "uniq_mids, inv_mid = np.unique(movie_ids, return_inverse=True)\n",
    "test_event_idx = np.array([\n",
    "    rng.choice(np.where(inv_mid == grp)[0], size=1)[0]\n",
    "    for grp in range(uniq_mids.size)\n",
    "], dtype=int)\n",
    "\n",
    "all_event_idx = np.arange(movie_ids.size)\n",
    "mask_test     = np.zeros_like(all_event_idx, dtype=bool)\n",
    "mask_test[test_event_idx] = True\n",
    "\n",
    "train_event_idx = all_event_idx[~mask_test]\n",
    "test_event_idx  = all_event_idx[ mask_test]\n",
    "\n",
    "# %% 3) BUILD CSR MOVIE×USER RATING MATRIX ON TRAIN\n",
    "\n",
    "uniq_uids = np.unique(user_ids)\n",
    "n_movies  = uniq_mids.size\n",
    "n_users   = uniq_uids.size\n",
    "\n",
    "mid_to_idx = {mid: i for i, mid in enumerate(uniq_mids)}\n",
    "uid_to_idx = {uid: j for j, uid in enumerate(uniq_uids)}\n",
    "\n",
    "# prepare arrays for COO construction\n",
    "rows = [mid_to_idx[movie_ids[i]] for i in train_event_idx]\n",
    "cols = [uid_to_idx[user_ids[i]]  for i in train_event_idx]\n",
    "data = ratings[train_event_idx]\n",
    "\n",
    "rating_sparse_train = coo_matrix(\n",
    "    (data, (rows, cols)),\n",
    "    shape=(n_movies, n_users)\n",
    ").tocsr()\n",
    "\n",
    "# %% 4) GENERATE PER-MOVIE FEATURES (ON TRAIN DATA)\n",
    "\n",
    "# 4a) rating count per movie\n",
    "rating_counts = rating_sparse_train.getnnz(axis=1)\n",
    "\n",
    "# 4b) sum of ratings per movie, then average\n",
    "sum_ratings   = rating_sparse_train.sum(axis=1).A1\n",
    "avg_ratings   = sum_ratings / rating_counts\n",
    "\n",
    "# 4c) release year per movie, impute missing\n",
    "release_years = np.array([year_map.get(mid, np.nan) for mid in uniq_mids],\n",
    "                         dtype=float)\n",
    "valid_years   = release_years[np.isfinite(release_years)]\n",
    "year_med      = np.median(valid_years) if valid_years.size else datetime.now().year\n",
    "release_years = np.where(np.isfinite(release_years),\n",
    "                         release_years,\n",
    "                         year_med)\n",
    "\n",
    "# stack into (n_movies × 3)\n",
    "movie_feats = np.vstack([avg_ratings, rating_counts, release_years]).T\n",
    "\n",
    "# %% 5) CLUSTERING FOR DIMENSIONALITY REDUCTION\n",
    "# prepare the 2D input (avg_rating, count)\n",
    "cluster_input = StandardScaler().fit_transform(movie_feats[:, :2])\n",
    "\n",
    "cluster_algos = {\n",
    "    \"KMeans\":            KMeans(n_clusters=30, random_state=random_state),\n",
    "    \"Agglomerative\":     AgglomerativeClustering(n_clusters=30),\n",
    "    \"DBSCAN\":            DBSCAN(eps=0.5, min_samples=5),\n",
    "    \"GaussianMixture\":   GaussianMixture(n_components=30, random_state=random_state)\n",
    "}\n",
    "\n",
    "print(\"\\nClustering performance on movie_feats:\")\n",
    "for name, algo in cluster_algos.items():\n",
    "    if name == \"GaussianMixture\":\n",
    "        labels = algo.fit(cluster_input).predict(cluster_input)\n",
    "    else:\n",
    "        labels = algo.fit_predict(cluster_input)\n",
    "\n",
    "    # skip metrics if only one cluster or too many small clusters\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    if n_clusters <= 1:\n",
    "        print(f\" - {name}: only {n_clusters} cluster(s), skipping metrics\")\n",
    "        continue\n",
    "\n",
    "    sil  = silhouette_score(cluster_input, labels)\n",
    "    ch   = calinski_harabasz_score(cluster_input, labels)\n",
    "    db   = davies_bouldin_score(cluster_input, labels)\n",
    "    print(f\" - {name:15s}  clusters={n_clusters:2d}  \"\n",
    "          f\"Silhouette={sil:.3f}  CH={ch:.1f}  DB={db:.3f}\")\n",
    "\n",
    "# choose KMeans labels for downstream:\n",
    "clusters = cluster_algos[\"KMeans\"].fit_predict(cluster_input)\n",
    "\n",
    "# %% 6) BUILD EVENT-LEVEL FEATURE ARRAYS & MODEL\n",
    "\n",
    "# map each event to its movie’s cluster\n",
    "train_movie_idx = inv_mid[train_event_idx]\n",
    "test_movie_idx  = inv_mid[test_event_idx]\n",
    "\n",
    "train_clusters_event = clusters[train_movie_idx].reshape(-1, 1)\n",
    "test_clusters_event  = clusters[test_movie_idx ].reshape(-1, 1)\n",
    "\n",
    "# one-hot encode cluster IDs\n",
    "ohe             = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "cluster_ohe_tr  = ohe.fit_transform(train_clusters_event)\n",
    "cluster_ohe_te  = ohe.transform(test_clusters_event)\n",
    "\n",
    "# scale release year per event\n",
    "ry_train        = release_years[train_movie_idx].reshape(-1, 1)\n",
    "ry_test         = release_years[test_movie_idx ].reshape(-1, 1)\n",
    "scaler_yr       = StandardScaler()\n",
    "ry_tr_scaled    = scaler_yr.fit_transform(ry_train)\n",
    "ry_te_scaled    = scaler_yr.transform(ry_test)\n",
    "\n",
    "# assemble design matrices\n",
    "X_train = np.hstack([cluster_ohe_tr, ry_tr_scaled])\n",
    "y_train = ratings[train_event_idx]\n",
    "\n",
    "X_test  = np.hstack([cluster_ohe_te, ry_te_scaled])\n",
    "y_test  = ratings[test_event_idx]\n",
    "\n",
    "#%% 7) fit model \n",
    "\n",
    "# linear regression\n",
    "lr    = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "rmse  = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Linear Regression RMSE: {rmse:.4f}\")\n",
    "subprocess.run([\"say\", \"Yo  come look at these results!! script has finished running\"])\n",
    "'''\n",
    "# use the original integer ratings as class labels\n",
    "y_train_cls = y_train\n",
    "y_test_cls  = y_test\n",
    "\n",
    "# 6a) Linear SVM\n",
    "svc = LinearSVC(\n",
    "    random_state=random_state,\n",
    "    max_iter=1000,      # increase if needed\n",
    "    tol=1e-4\n",
    ")\n",
    "svc.fit(X_train, y_train_cls)\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "acc_svc    = accuracy_score(y_test_cls, y_pred_svc)\n",
    "print(f\"LinearSVC Accuracy: {acc_svc:.4f}\")\n",
    "print(classification_report(y_test_cls, y_pred_svc))\n",
    "\n",
    "# 6b) SGDClassifier (hinge loss = linear SVM)\n",
    "sgd = SGDClassifier(\n",
    "    loss=\"hinge\",\n",
    "    random_state=random_state,\n",
    "    max_iter=1000,\n",
    "    tol=1e-4\n",
    ")\n",
    "sgd.fit(X_train, y_train_cls)\n",
    "y_pred_sgd = sgd.predict(X_test)\n",
    "acc_sgd    = accuracy_score(y_test_cls, y_pred_sgd)\n",
    "print(f\"SGDClassifier Accuracy: {acc_sgd:.4f}\")\n",
    "print(classification_report(y_test_cls, y_pred_sgd))\n",
    "\n",
    "subprocess.run([\"say\", \"Yo  come look at these results!! script has finished running\"])\n",
    "'''\n",
    "\n",
    "import nbformat\n",
    "\n",
    "# read—nbformat will skip non-JSON lines\n",
    "nb = nbformat.read('FMLcapstone.ipynb', as_version=4)\n",
    "\n",
    "# write out clean JSON\n",
    "nbformat.write(nb, 'FMLcapstone_fixed.ipynb')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
