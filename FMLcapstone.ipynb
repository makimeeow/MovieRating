#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Pipeline with one–rating-per-movie test split using CSR for the rating matrix
Features include:
- Movie-level: avg_rating, rating_count, release_year
- User-level: user_mean, user_count, user_cluster
- Time-since-release (days)
- Title embeddings (TF-IDF)
- Temporal context: day-of-week, month (cyclical)
- Bias term: global + user_bias + movie_bias
- Cross-feature: movie_cluster × release_year.
"""

import numpy as np
from datetime import datetime
from scipy.sparse import coo_matrix
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import Ridge, Lasso, SGDRegressor, LinearRegression, RidgeCV
from sklearn.svm import LinearSVC
from scipy.sparse import csr_matrix, hstack
from sklearn.metrics import (
    mean_squared_error,
    accuracy_score,
    classification_report,
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score
)
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.feature_extraction.text import TfidfVectorizer
from xgboost import XGBRegressor
from sklearn.model_selection import RandomizedSearchCV
import subprocess

# Reproducibility
random_state = 13119140
rng = np.random.default_rng(random_state)

# %% 1) LOAD RAW DATA
movie_ids, user_ids, ratings, dates = [], [], [], []
with open("/Users/serenahan/Downloads/dataSet/data.txt", "r") as f:
    current_mid = None
    for line in f:
        line = line.strip()
        if line.endswith(":"):
            current_mid = int(line[:-1])
        else:
            u, r, d = line.split(",")
            movie_ids.append(current_mid)
            user_ids.append(int(u))
            ratings.append(int(r))
            dates.append(np.datetime64(d))
movie_ids = np.array(movie_ids, dtype=int)
user_ids  = np.array(user_ids,  dtype=int)
ratings   = np.array(ratings,   dtype=int)
dates     = np.array(dates,     dtype="datetime64[D]")

# %% 1b) LOAD MOVIE METADATA
mids, rel_years, rel_dates, titles = [], [], [], []
with open("/Users/serenahan/Downloads/dataSet/movieTitles.csv", "r", encoding="utf-8") as f:
    for line in f:
        mid_str, date_str, title_str = line.rstrip("\n").split(",", 2)
        try:
            mid = int(mid_str)
        except ValueError:
            continue
        try:
            yr = datetime.strptime(date_str, "%Y-%m-%d").year
            rd = np.datetime64(date_str)
        except:
            yr = np.nan
            rd = np.datetime64("1970-01-01")
        mids.append(mid); rel_years.append(yr); rel_dates.append(rd)
        titles.append(title_str.strip())
        
mids      = np.array(mids,      dtype=int)
rel_years = np.array(rel_years, dtype=float)
rel_dates = np.array(rel_dates, dtype="datetime64[D]")
titles    = np.array(titles,    dtype=object)

# Lookup maps
year_map  = {mid: yr for mid, yr in zip(mids, rel_years)}
date_map  = {mid: rd for mid, rd in zip(mids, rel_dates)}
title_map = {mid: ti for mid, ti in zip(mids, titles)}

# %% 2) SPLIT EVENTS INTO TRAIN/TEST
uniq_mids, inv_mid = np.unique(movie_ids, return_inverse=True)
test_event_idx = np.array([
    rng.choice(np.where(inv_mid == gid)[0], size=1)[0]
    for gid in range(uniq_mids.size)
], dtype=int)
all_idx = np.arange(movie_ids.size)
mask_test = np.zeros_like(all_idx, dtype=bool)
mask_test[test_event_idx] = True
train_event_idx = all_idx[~mask_test]
test_event_idx  = all_idx[ mask_test]

# %% 3) BUILD CSR MOVIE×USER MATRIX ON TRAIN
uniq_uids = np.unique(user_ids)
n_movies, n_users = uniq_mids.size, uniq_uids.size
mid_to_idx = {mid: i for i, mid in enumerate(uniq_mids)}
uid_to_idx = {uid: j for j, uid in enumerate(uniq_uids)}

rows = [mid_to_idx[movie_ids[i]] for i in train_event_idx]
cols = [uid_to_idx[user_ids[i]]  for i in train_event_idx]
data = ratings[train_event_idx]
rating_sparse_train = coo_matrix(
    (data, (rows, cols)),
    shape=(n_movies, n_users)
).tocsr()

# %% 4) MOVIE-LEVEL FEATURES
rating_counts = rating_sparse_train.getnnz(axis=1)
sum_ratings   = rating_sparse_train.sum(axis=1).A1
avg_ratings   = sum_ratings / rating_counts
release_years = np.array([year_map[mid] for mid in uniq_mids], dtype=float)
valid_years   = release_years[np.isfinite(release_years)]
year_med      = np.median(valid_years) if valid_years.size else datetime.now().year
release_years = np.where(np.isfinite(release_years), release_years, year_med)

# %% 4.5) ENRICH FEATURES (NO CLUSTERING)

# -- User-level features --
user_counts = rating_sparse_train.getnnz(axis=0)
user_sums   = rating_sparse_train.sum(axis=0).A1
global_mean = ratings[train_event_idx].mean()
user_means  = np.divide(
    user_sums,
    user_counts,
    out=np.full_like(user_sums, global_mean, dtype=float),
    where=(user_counts > 0)
)

train_uidx = np.array([uid_to_idx[user_ids[i]] for i in train_event_idx])
test_uidx  = np.array([uid_to_idx[user_ids[i]] for i in test_event_idx])

user_mean_tr  = user_means[train_uidx].reshape(-1,1)
user_count_tr = user_counts[train_uidx].reshape(-1,1)
user_mean_te  = user_means[test_uidx].reshape(-1,1)
user_count_te = user_counts[test_uidx].reshape(-1,1)

# -- Release-year scaled --
ry_train    = release_years[inv_mid[train_event_idx]].reshape(-1,1)
ry_test     = release_years[inv_mid[test_event_idx]].reshape(-1,1)
scaler_yr   = StandardScaler()
ry_tr_scaled = scaler_yr.fit_transform(ry_train)
ry_te_scaled = scaler_yr.transform(ry_test)

# -- Time-since-release (days) --
movie_release_dates = np.array([date_map[mid] for mid in uniq_mids])
tr_rd = movie_release_dates[inv_mid[train_event_idx]]
te_rd = movie_release_dates[inv_mid[test_event_idx]]
age_days_tr = (dates[train_event_idx] - tr_rd).astype('timedelta64[D]').astype(int).reshape(-1,1)
age_days_te = (dates[test_event_idx]  - te_rd ).astype('timedelta64[D]').astype(int).reshape(-1,1)

# -- Title TF-IDF embeddings --
titles_uniq = [title_map[mid] for mid in uniq_mids]
tfidf       = TfidfVectorizer(max_features=100)
tfidf_mat   = tfidf.fit_transform(titles_uniq)
title_tr    = tfidf_mat[inv_mid[train_event_idx]].toarray()
title_te    = tfidf_mat[inv_mid[test_event_idx]].toarray()

# -- Temporal context --
def to_pydate(d64):
    days = d64.astype('datetime64[D]').astype(int)
    return datetime.utcfromtimestamp(days * 86400).date()
py_dates = np.array([to_pydate(d) for d in dates])
dow      = np.array([d.weekday() for d in py_dates], dtype=int)
mon      = np.array([d.month    for d in py_dates], dtype=int)
dow_tr, mon_tr = dow[train_event_idx].reshape(-1,1), mon[train_event_idx].reshape(-1,1)
dow_te, mon_te = dow[test_event_idx].reshape(-1,1),  mon[test_event_idx].reshape(-1,1)
sin_mon_tr = np.sin(2 * np.pi * (mon_tr-1) / 12)
cos_mon_tr = np.cos(2 * np.pi * (mon_tr-1) / 12)
sin_mon_te = np.sin(2 * np.pi * (mon_te-1) / 12)
cos_mon_te = np.cos(2 * np.pi * (mon_te-1) / 12)
'''
# -- Bias term --
user_bias   = user_means - global_mean
movie_bias  = avg_ratings - global_mean
bias_tr     = (global_mean + user_bias[train_uidx] + movie_bias[inv_mid[train_event_idx]]).reshape(-1,1)
bias_te     = (global_mean + user_bias[test_uidx]  + movie_bias[inv_mid[test_event_idx]] ).reshape(-1,1)
'''
# %% 5) BUILD MATRICES

# define y
y_train = ratings[train_event_idx]
y_test  = ratings[test_event_idx]

# build sparse feature blocks
A = csr_matrix(ry_tr_scaled)
B = csr_matrix(user_mean_tr)
C = csr_matrix(user_count_tr)
D = csr_matrix(age_days_tr)
E = tfidf_mat[inv_mid[train_event_idx]]    # already csr
F = csr_matrix(dow_tr)
G = csr_matrix(mon_tr)
H = csr_matrix(sin_mon_tr)
I = csr_matrix(cos_mon_tr)

X_train= hstack([A, B, C, D, E, F, G, H, I], format="csr")

A_te = csr_matrix(ry_te_scaled)
B_te = csr_matrix(user_mean_te)
C_te = csr_matrix(user_count_te)
D_te = csr_matrix(age_days_te)
E_te = tfidf_mat[inv_mid[test_event_idx]]
F_te = csr_matrix(dow_te)
G_te = csr_matrix(mon_te)
H_te = csr_matrix(sin_mon_te)
I_te = csr_matrix(cos_mon_te)

X_test = hstack([A_te, B_te, C_te, D_te, E_te, F_te, G_te, H_te, I_te], format="csr")

# %% FIT & EVALUATE MODELS

# %% lienar regression
lr = LinearRegression()
lr.fit(X_train, y_train)
print(f"Linear Regression RMSE: {np.sqrt(mean_squared_error(y_test, lr.predict(X_test))):.4f}")

# %% ridge 
alphas = [0.1, 1.0, 10.0, 100.0]
results = {}

model = Ridge(random_state=random_state)

for alpha in alphas:
    model.alpha = alpha      # update regularization strength
    model.fit(X_train, y_train)
    
    preds = model.predict(X_test)
    rmse  = np.sqrt(mean_squared_error(y_test, preds))
    results[alpha] = rmse

# report
for alpha, rmse in results.items():
    print(f"alpha={alpha:>6}  → RMSE: {rmse:.4f}")
# %%  LGB
from sklearn.linear_model import RidgeCV

from lightgbm import LGBMRegressor


lgb = LGBMRegressor(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=5,
    random_state=random_state,
    n_jobs=-1
)
lgb.fit(X_train, y_train)
preds = lgb.predict(X_test)
print("LightGBM RMSE:", np.sqrt(mean_squared_error(y_test, preds)))

#%% XGBoost

param_dist = {
    "n_estimators":  [100, 300, 600],
    "learning_rate": [0.05, 0.1],
    "max_depth":     [5, 8, 11],
}

xgb = XGBRegressor(
    random_state=random_state,
    verbosity=0,
    n_jobs=1   # only one worker
)

rs = RandomizedSearchCV(
    xgb,
    param_distributions=param_dist,
    n_iter=10,         # try 10 combos instead of 30
    cv=2,              # 2‐fold CV
    scoring="neg_mean_squared_error",
    verbose=1,
    random_state=random_state,
    n_jobs=1           # no parallelism
)

rs.fit(X_train, y_train)
 
best = rs.best_estimator_
print("Best params:", rs.best_params_)

preds = best.predict(X_test)
rmse  = np.sqrt(mean_squared_error(y_test, preds))
print(f"Tuned XGB RMSE: {rmse:.4f}")


'''
#%% dimensionality reduction
from sklearn.decomposition import TruncatedSVD

svd = TruncatedSVD(n_components=20, random_state=random_state)
# fit on the full title TF-IDF
svd_title = svd.fit_transform(tfidf_mat)  # shape (n_movies, 20)

# Build reduced design matrices
title_tr_svd = svd_title[inv_mid[train_event_idx]]
title_te_svd = svd_title[inv_mid[test_event_idx]]

# Re-assemble X_train_svd / X_test_svd replacing the 100-dim block
from scipy.sparse import hstack, csr_matrix

blocks_tr = [
    csr_matrix(ry_tr_scaled),
    csr_matrix(user_mean_tr),
    csr_matrix(user_count_tr),
    csr_matrix(age_days_tr),
    csr_matrix(dow_tr), csr_matrix(mon_tr),
    csr_matrix(sin_mon_tr), csr_matrix(cos_mon_tr)
]
X_train_sparse = hstack(blocks_tr, format="csr")
X_train_svd    = np.hstack([X_train_sparse.toarray(), title_tr_svd])

blocks_te = [
    csr_matrix(ry_te_scaled),
    csr_matrix(user_mean_te),
    csr_matrix(user_count_te),
    csr_matrix(age_days_te),
    csr_matrix(dow_te), csr_matrix(mon_te),
    csr_matrix(sin_mon_te), csr_matrix(cos_mon_te)
]
X_test_svd = np.hstack([hstack(blocks_te,format="csr").toarray(), title_te_svd])

#%% SGDRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import SGDRegressor

pipe = make_pipeline(
    PolynomialFeatures(degree=2, interaction_only=True, include_bias=False),
    SGDRegressor(max_iter=100, tol=1e-2, random_state=random_state)
)
pipe.fit(X_train_svd, y_train)
rmse = np.sqrt(mean_squared_error(y_test, pipe.predict(X_test_svd)))
print("Poly-SGD RMSE:", rmse)

'''
subprocess.run(["say", "Yo  come look at these results!! script has finished running"])
